{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d03075e9-668f-48e5-a669-b136b77a1aac",
      "metadata": {},
      "source": [
        "## Welcome to the FLAIR-GG Coordinates by species Analytics Notebook\n",
        "\n",
        "Please run the first cell to set-up the analytics environment\n",
        "\n",
        "In the second cell, we have pre-filled a basic analytics, to show you how to access and manipulate the data that was retrieved from the FLAIR-GG Virtual Platform.  \n",
        "\n",
        "Fill in the \"key = 'XXXXXXX' with the secret key for your federated exploration output, and then... go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfe0cbee-b01d-46a0-b529-9685621a233b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pyodide_kernel\n",
        "\n",
        "%pip install ipywidgets==8.1.1\n",
        "%pip install ipyevents==2.0.2\n",
        "%pip install ipympl==0.9.4\n",
        "%pip install ipycanvas==0.13.2\n",
        "%pip install ipyleaflet==0.18.2\n",
        "%pip install plotly==5.24.0\n",
        "%pip install bqplot==0.12.45\n",
        "%pip install altair\n",
        "%pip install pandas\n",
        "%pip install requests\n",
        "%pip install seaborn\n",
        "%pip install matplotlib\n",
        "%pip install geopandas\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import altair as alt\n",
        "import pandas as pd\n",
        "import pyodide_http\n",
        "import ssl\n",
        "import json\n",
        "import requests\n",
        "import urllib3\n",
        "import urllib.parse\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d531371-85ca-417d-b279-9fa94d48bf8a",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import io   \n",
        "\n",
        "key = \"XXXXXXXX\"   # your secret key from the FLAIR-GG Virtual Platform\n",
        "# key = \"630e5b25-bc4e-4568-833c-8b8bc7303dcb\"   # a valid SPARQL results output, if you need it :-)\n",
        "\n",
        "url = \"https://bgv.cbgp.upm.es/DAV/home/LDP/FLAIR/{}\".format(key)\n",
        "\n",
        "\n",
        "response = requests.get(url)\n",
        "response = json.loads(response.content)\n",
        "# print(response)\n",
        "for provider in response.keys():\n",
        "    print(\"Provider: {}\".format(provider))\n",
        "    data = response[provider]\n",
        "    df = pd.read_csv(io.StringIO(data), sep=\",\")    \n",
        "    df\n",
        "    print(df)\n",
        "    print(\"\\n\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9b2718-86c8-4467-95bc-1680dee0b995",
      "metadata": {},
      "source": [
        "Let's remove entries that contain a subspecies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08848f6d-c693-4a50-9c81-d795b1e7c5f1",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df = df[~df[\"scientific_name\"].str.contains(\"subsp.\")]\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0408ca7-8912-41f1-a9b4-30a0e1604d4c",
      "metadata": {},
      "source": [
        "The coordinates are in a different format, let's transform them to decimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0338a204-f3a4-4170-80cd-7f4f08573088",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def dms_to_decimal(coord):\n",
        "    # Example input: \"3408--N\" or \"00203--W\"\n",
        "    coord = coord.strip()\n",
        "    # split numeric part and hemisphere\n",
        "    hemisphere = coord[-1]\n",
        "    numbers = coord[:-3]  # remove \"--X\"\n",
        "    \n",
        "    # degrees are all but last two digits, minutes are last two\n",
        "    deg = int(numbers[:-2])\n",
        "    minutes = int(numbers[-2:])\n",
        "    \n",
        "    decimal = deg + minutes / 60\n",
        "    \n",
        "    if hemisphere in [\"S\", \"W\"]:\n",
        "        decimal = -decimal\n",
        "    \n",
        "    return decimal\n",
        "\n",
        "df[\"decimalLatitude\"] = df[\"latitude\"].apply(dms_to_decimal)\n",
        "df[\"decimalLongitude\"] = df[\"longitude\"].apply(dms_to_decimal)\n",
        "df_germplasm = df\n",
        "print (df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b4ae29b-3fac-483c-99e2-10721b8fcb6c",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Helper: read shapefile/geojson and ensure CRS\n",
        "# ================================\n",
        "def read_shapefile(path):\n",
        "    return gpd.read_file(path).to_crs(\"EPSG:4326\")\n",
        "\n",
        "# ================================\n",
        "# Helper: filter points within a community polygon\n",
        "# ================================\n",
        "def filter_within_community(gdf_points, communities_file, community_name):\n",
        "    gdf_comm = read_shapefile(communities_file)\n",
        "    comm = gdf_comm[gdf_comm[\"NAME_1\"].str.strip().str.lower() == community_name.strip().lower()]\n",
        "    if comm.empty:\n",
        "        raise ValueError(f\"Community '{community_name}' not found!\")\n",
        "    return gdf_points[gdf_points.within(comm.geometry.union_all())]\n",
        "\n",
        "# ================================\n",
        "# 1️⃣ Query GBIF directly using requests\n",
        "# ================================\n",
        "import asyncio\n",
        "from pyodide.http import pyfetch\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import json\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "async def fetch_gbif_page(url, params):\n",
        "    # Build full URL\n",
        "    from urllib.parse import urlencode\n",
        "    full_url = f\"{url}?{urlencode(params)}\"\n",
        "    headers = {\"User-Agent\": \"Python-GBIF-Client/1.0\"}\n",
        "    try:\n",
        "        response = await pyfetch(full_url, method=\"GET\", headers=headers)\n",
        "        if not response.ok:\n",
        "            raise Exception(f\"HTTP error: {response.status}\")\n",
        "        raw_content = await response.text()\n",
        "        content_length = response.headers.get('content-length')\n",
        "        actual_length = len(raw_content)\n",
        "        # warnings.warn(f\"Response length: {actual_length} bytes\")\n",
        "        if content_length and int(content_length) > actual_length:\n",
        "            warnings.warn(f\"Warning: Response truncated! Expected {content_length} bytes, got {actual_length}\")\n",
        "        # warnings.warn(f\"Raw response (first 1000 chars): {raw_content[:1000]}\")\n",
        "        # warnings.warn(f\"Raw response (last 100 chars): {raw_content[-100:]}\")\n",
        "        try:\n",
        "            data = json.loads(raw_content)\n",
        "        except json.JSONDecodeError as e:\n",
        "            warnings.warn(f\"JSON decode error at position {e.pos}: {e.msg}\")\n",
        "            warnings.warn(\"Consider inspecting raw_content manually\")\n",
        "            raise\n",
        "        # warnings.warn(f\"Has endOfRecords: {data.get('endOfRecords', False)}\")\n",
        "        # warnings.warn(f\"Result count: {data.get('count', 0)}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Request error: {e}\")\n",
        "        raise\n",
        "\n",
        "async def query_gbif_by_community_bbox(species_name, community_name, communities_file, limit=500):\n",
        "    try:\n",
        "        gdf_comm = read_shapefile(communities_file)\n",
        "        warnings.warn(f\"Shapefile CRS: {gdf_comm.crs}\")\n",
        "        if gdf_comm.crs is None:\n",
        "            warnings.warn(\"Shapefile has no CRS, setting to EPSG:4326\")\n",
        "            gdf_comm = gdf_comm.set_crs(\"EPSG:4326\", allow_override=True)\n",
        "        community = gdf_comm[gdf_comm[\"NAME_1\"].str.strip().str.lower() == community_name.strip().lower()]\n",
        "        if community.empty:\n",
        "            raise ValueError(f\"Community '{community_name}' not found!\")\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error reading shapefile or finding community: {e}\")\n",
        "        return gpd.GeoDataFrame(columns=[\"decimalLatitude\", \"decimalLongitude\"], geometry=[])\n",
        "\n",
        "    # Bounding box for GBIF query (shrink to reduce results)\n",
        "    try:\n",
        "        minx, miny, maxx, maxy = community.total_bounds\n",
        "        # Shrink box by 20% to further reduce result count\n",
        "        dx, dy = (maxx - minx) * 0.2, (maxy - miny) * 0.2\n",
        "        minx, miny, maxx, maxy = minx + dx, miny + dy, maxx - dx, maxy - dy\n",
        "        polygon_wkt = f\"POLYGON(({minx} {miny}, {minx} {maxy}, {maxx} {maxy}, {maxx} {miny}, {minx} {miny}))\"\n",
        "        warnings.warn(f\"Bounding box: {polygon_wkt}\")\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error creating bounding box: {e}\")\n",
        "        return gpd.GeoDataFrame(columns=[\"decimalLatitude\", \"decimalLongitude\"], geometry=[])\n",
        "\n",
        "    url = \"https://api.gbif.org/v1/occurrence/search\"\n",
        "    params = {\n",
        "        \"scientificName\": species_name,\n",
        "        \"geometry\": polygon_wkt,\n",
        "        \"limit\": min(limit, 20),  # Further reduced to 20\n",
        "        \"hasCoordinate\": True,\n",
        "        \"fields\": \"decimalLatitude,decimalLongitude,country,stateProvince,municipality\"\n",
        "    }\n",
        "\n",
        "    all_results = []\n",
        "    offset = 0\n",
        "    total_limit = limit\n",
        "\n",
        "    while len(all_results) < total_limit:\n",
        "        params[\"offset\"] = offset\n",
        "        try:\n",
        "            data = await fetch_gbif_page(url, params)\n",
        "            results = data.get(\"results\", [])\n",
        "            all_results.extend(results)\n",
        "            if data.get(\"endOfRecords\", False) or not results:\n",
        "                break\n",
        "            offset += params[\"limit\"]\n",
        "            await asyncio.sleep(1.5)  # Async sleep\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Failed to fetch page at offset {offset}: {e}\")\n",
        "            break\n",
        "\n",
        "    df = pd.DataFrame(all_results[:total_limit])\n",
        "    if df.empty:\n",
        "        warnings.warn(\"No GBIF occurrences found.\")\n",
        "        empty_gdf = gpd.GeoDataFrame(columns=[\"decimalLatitude\", \"decimalLongitude\"], geometry=[])\n",
        "        empty_gdf.crs = \"EPSG:4326\"  # Set CRS explicitly to avoid PROJ issues\n",
        "        return empty_gdf\n",
        "\n",
        "    df = df.reindex(columns=[\"decimalLatitude\", \"decimalLongitude\", \"country\", \"stateProvince\", \"municipality\"])\n",
        "    try:\n",
        "        gdf_occ = gpd.GeoDataFrame(\n",
        "            df,\n",
        "            geometry=[Point(xy) for xy in zip(df[\"decimalLongitude\"], df[\"decimalLatitude\"])],\n",
        "            crs=\"EPSG:4326\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error creating GeoDataFrame: {e}\")\n",
        "        empty_gdf = gpd.GeoDataFrame(columns=[\"decimalLatitude\", \"decimalLongitude\"], geometry=[])\n",
        "        empty_gdf.crs = \"EPSG:4326\"\n",
        "        return empty_gdf\n",
        "\n",
        "    # Filter exactly inside community polygon\n",
        "    try:\n",
        "        if not gdf_occ.empty:\n",
        "            gdf_occ = filter_within_community(gdf_occ, communities_file, community_name)\n",
        "        else:\n",
        "            warnings.warn(\"Skipping filter_within_community due to empty GeoDataFrame\")\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error filtering within community: {e}\")\n",
        "        empty_gdf = gpd.GeoDataFrame(columns=[\"decimalLatitude\", \"decimalLongitude\"], geometry=[])\n",
        "        empty_gdf.crs = \"EPSG:4326\"\n",
        "        return empty_gdf\n",
        "\n",
        "    return gdf_occ\n",
        "\n",
        "# To use in JupyterLite: await query_gbif_by_community_bbox(species_name, community_name, communities_file, limit=50)\n",
        "\n",
        "# ================================\n",
        "# 2️⃣ Assign province & municipality (levels 2 & 3)\n",
        "# ================================\n",
        "def assign_admin_levels(gdf_occurrences, provinces_file, municipalities_file):\n",
        "    gdf = gdf_occurrences.copy()\n",
        "    for shapefile, level_name, col_name in [\n",
        "        (provinces_file, \"NAME_2\", \"province_assigned\"),\n",
        "        (municipalities_file, \"NAME_3\", \"municipality_assigned\")\n",
        "    ]:\n",
        "        gdf_s = read_shapefile(shapefile)\n",
        "        gdf = gpd.sjoin(gdf, gdf_s, how=\"left\", predicate=\"within\")\n",
        "        gdf[col_name] = gdf[level_name]\n",
        "        if \"index_right\" in gdf.columns:\n",
        "            gdf = gdf.drop(columns=[\"index_right\"])\n",
        "    return gdf\n",
        "\n",
        "# ================================\n",
        "# 3️⃣ Coverage comparison\n",
        "# ================================\n",
        "def calculate_coverage_with_names(gdf_gbif, gdf_germplasm, plot=False):\n",
        "    # Provinces\n",
        "    gbif_provinces = set(gdf_gbif[\"province_assigned\"].dropna().unique())\n",
        "    germ_provinces = set(gdf_germplasm[\"province_assigned\"].dropna().unique())\n",
        "    province_coverage = (\n",
        "        len(germ_provinces & gbif_provinces) / len(gbif_provinces) * 100\n",
        "        if gbif_provinces else 0\n",
        "    )\n",
        "\n",
        "    # Municipalities\n",
        "    gbif_munis = set(gdf_gbif[\"municipality_assigned\"].dropna().unique())\n",
        "    germ_munis = set(gdf_germplasm[\"municipality_assigned\"].dropna().unique())\n",
        "    municipality_coverage = (\n",
        "        len(germ_munis & gbif_munis) / len(gbif_munis) * 100\n",
        "        if gbif_munis else 0\n",
        "    )\n",
        "\n",
        "    if plot:\n",
        "        labels = [\"Province coverage\", \"Municipality coverage\"]\n",
        "        values = [province_coverage, municipality_coverage]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 5))\n",
        "        bars = ax.bar(labels, values, color=[\"steelblue\", \"seagreen\"])\n",
        "\n",
        "        # --- dynamic ylim headroom ---\n",
        "        max_val = max(values)\n",
        "        ylim_top = max_val * 1.1 if max_val >= 95 else 100\n",
        "        ax.set_ylim(0, ylim_top)\n",
        "\n",
        "        ax.set_ylabel(\"Coverage (%)\")\n",
        "        ax.set_title(\"Coverage of Germplasm vs GBIF Occurrences\")\n",
        "\n",
        "        # Add value labels above bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.annotate(f\"{height:.1f}%\",\n",
        "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                        xytext=(0, 3),  # slight offset above bar\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha=\"center\", va=\"bottom\", fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return province_coverage, municipality_coverage, gbif_provinces, germ_provinces, gbif_munis, germ_munis\n",
        "\n",
        "# ================================\n",
        "# 4️⃣ Plot dataset\n",
        "# ================================\n",
        "def plot_dataset_separately(\n",
        "    gdf_occurrences,\n",
        "    provinces_file,\n",
        "    municipalities_file=None,\n",
        "    communities_file=None,\n",
        "    community_name=None,\n",
        "    title_prefix=\"Occurrences\"\n",
        "):\n",
        "    if gdf_occurrences.empty:\n",
        "        print(f\"No points to plot for {title_prefix}.\")\n",
        "        return\n",
        "\n",
        "    gdf_prov = read_shapefile(provinces_file)\n",
        "    gdf_muni = read_shapefile(municipalities_file) if municipalities_file else None\n",
        "    gdf_comm = read_shapefile(communities_file) if communities_file else None\n",
        "\n",
        "    # Color by province\n",
        "    provinces = gdf_occurrences[\"province_assigned\"].dropna().unique()\n",
        "    cmap = plt.get_cmap(\"tab20\")\n",
        "    province_colors = {prov: cmap(i % 20) for i, prov in enumerate(provinces)}\n",
        "    gdf_occurrences[\"color\"] = gdf_occurrences[\"province_assigned\"].map(province_colors)\n",
        "\n",
        "    # --- Make a single figure with 2 axes ---\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "    # --- Full map of Spain ---\n",
        "    gdf_prov.boundary.plot(ax=ax1, color=\"black\", linewidth=0.5)\n",
        "    if gdf_muni is not None:\n",
        "        gdf_muni.boundary.plot(ax=ax1, color=\"gray\", linewidth=0.2, alpha=0.3)\n",
        "    for prov, color in province_colors.items():\n",
        "        subset = gdf_occurrences[gdf_occurrences[\"province_assigned\"] == prov]\n",
        "        subset.plot(ax=ax1, color=color, markersize=30, alpha=0.7, label=prov)\n",
        "    if gdf_comm is not None and community_name:\n",
        "        comm = gdf_comm[gdf_comm[\"NAME_1\"].str.strip().str.lower() == community_name.strip().lower()]\n",
        "        if not comm.empty:\n",
        "            comm.boundary.plot(ax=ax1, color=\"red\", linewidth=2)\n",
        "    ax1.set_title(f\"{title_prefix} in Spain\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Longitude\")\n",
        "    ax1.set_ylabel(\"Latitude\")\n",
        "    ax1.legend(title=\"Province\", fontsize=9)\n",
        "\n",
        "    # --- Zoomed-in community map ---\n",
        "    if community_name and gdf_comm is not None:\n",
        "        comm = gdf_comm[gdf_comm[\"NAME_1\"].str.strip().str.lower() == community_name.strip().lower()]\n",
        "        if comm.empty:\n",
        "            print(f\"Community '{community_name}' not found for zoomed-in map.\")\n",
        "        else:\n",
        "            minx, miny, maxx, maxy = comm.total_bounds\n",
        "            ax2.set_xlim(minx - 0.1, maxx + 0.1)\n",
        "            ax2.set_ylim(miny - 0.1, maxy + 0.1)\n",
        "            gdf_prov.boundary.plot(ax=ax2, color=\"black\", linewidth=0.5)\n",
        "            if gdf_muni is not None:\n",
        "                gdf_muni.boundary.plot(ax=ax2, color=\"gray\", linewidth=0.2, alpha=0.3)\n",
        "            comm.boundary.plot(ax=ax2, color=\"red\", linewidth=2)\n",
        "            gdf_comm_points = gdf_occurrences[gdf_occurrences.within(comm.geometry.union_all())] if not gdf_occurrences.empty else gdf_occurrences\n",
        "            for prov, color in province_colors.items():\n",
        "                subset = gdf_comm_points[gdf_comm_points[\"province_assigned\"] == prov]\n",
        "                subset.plot(ax=ax2, color=color, markersize=30, alpha=0.7, label=prov)\n",
        "            ax2.set_title(f\"{title_prefix} in {community_name}\", fontsize=14)\n",
        "            ax2.set_xlabel(\"Longitude\")\n",
        "            ax2.set_ylabel(\"Latitude\")\n",
        "            ax2.legend(title=\"Province\", fontsize=9)\n",
        "    else:\n",
        "        ax2.axis(\"off\")  # hide if no zoom map\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 5️⃣ Example workflow\n",
        "# ================================\n",
        "if __name__ == \"__main__\":\n",
        "    species = \"Ecballium elaterium\"\n",
        "    community = \"Comunidad de Madrid\"\n",
        "\n",
        "    communities_file = \"shapefiles/gadm41_ESP_1.shp\"\n",
        "    provinces_file = \"shapefiles/gadm41_ESP_2.shp\"\n",
        "    municipalities_file = \"shapefiles/gadm41_ESP_3.shp\"\n",
        "\n",
        "    # --- GBIF ---\n",
        "    gdf_gbif = await query_gbif_by_community_bbox(species, community, communities_file, limit=500)\n",
        "    gdf_gbif = assign_admin_levels(gdf_gbif, provinces_file, municipalities_file)\n",
        "\n",
        "    # --- Germplasm bank ---\n",
        "    gdf_germplasm = gpd.GeoDataFrame(\n",
        "        df_germplasm,\n",
        "        geometry=[Point(xy) for xy in zip(df[\"decimalLongitude\"], df[\"decimalLatitude\"])],\n",
        "        crs=\"EPSG:4326\"\n",
        "    )\n",
        "    gdf_germplasm = filter_within_community(gdf_germplasm, communities_file, community)\n",
        "    gdf_germplasm = assign_admin_levels(gdf_germplasm, provinces_file, municipalities_file)\n",
        "\n",
        "    # --- Coverage comparison ---\n",
        "    province_cov, municipality_cov, gbif_provs, germ_provs, gbif_munis, germ_munis = calculate_coverage_with_names(gdf_gbif, gdf_germplasm, plot=True)\n",
        "    print(f\"Germplasm bank covers {province_cov:.1f}% of GBIF provinces\")\n",
        "    print(f\"GBIF provinces: {gbif_provs}\")\n",
        "    print(f\"Germplasm provinces: {germ_provs}\")\n",
        "    print(f\"Germplasm bank covers {municipality_cov:.1f}% of GBIF municipalities\")\n",
        "    print(f\"GBIF municipalities: {gbif_munis}\")\n",
        "    print(f\"Germplasm municipalities: {germ_munis}\")\n",
        "\n",
        "    # --- Plots ---\n",
        "    plot_dataset_separately(\n",
        "        gdf_gbif,\n",
        "        provinces_file,\n",
        "        municipalities_file,\n",
        "        communities_file,\n",
        "        community_name=community,\n",
        "        title_prefix=\"GBIF occurrences\"\n",
        "    )\n",
        "    plot_dataset_separately(\n",
        "        gdf_germplasm,\n",
        "        provinces_file,\n",
        "        municipalities_file,\n",
        "        communities_file,\n",
        "        community_name=community,\n",
        "        title_prefix=\"Germplasm bank occurrences\"\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
